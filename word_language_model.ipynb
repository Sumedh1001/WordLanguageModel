{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word Language Model**\n",
    "\n",
    "Welcome to the language model example on Skafos! The purpose of this notebook is to get you going end-to-end and show you how to create a custom model outside of our quickstart models. Below we will do the following:\n",
    "\n",
    "1. Load Yelp review text data.\n",
    "2. Build a word-level, neural network language model.\n",
    "3. Convert the model to CoreML format and save it to the Skafos framework.\n",
    "\n",
    "The code in this example was adapted from this [**this article**](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) and follows along with [**this blog post**]() (*coming soon*) that we wrote to help guide you through it.\n",
    "\n",
    "---\n",
    "\n",
    "Execute each cell one-by-one, by selecting the cell and do one of the following:\n",
    "\n",
    "-  Clicking the \"play\" button at the top of this frame.\n",
    "-  Typing 'Control + Enter' or 'Shift + Enter'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prior to running any code below**\n",
    "Make sure you have installed all python dependencies in the JLab session before continuing. Open up the terminal and type:\n",
    "```bash\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "Once you've done that - restart the kernel for this notebook by hitting the clockwise arrow at the top of this panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries - if imports fail, make sure you have installed all dependencies in the requirements.txt\n",
    "import json\n",
    "import string\n",
    "from pickle import dump\n",
    "\n",
    "from numpy import array\n",
    "import coremltools\n",
    "import turicreate as tc\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from skafossdk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few helper functions\n",
    "\n",
    "# End of sentence tag\n",
    "eos = \"<eos>\"\n",
    "\n",
    "# Convert text entries into a big text blob\n",
    "def parse_text(data):\n",
    "    full_text = \"\"\n",
    "    for text in data:\n",
    "        entry = text.replace(\"\\n\", \"\").replace(\"\\'\", \"\").replace(\".\", f\" {eos} \")\n",
    "        full_text += entry\n",
    "    return full_text\n",
    "    \n",
    "# Turn a text blob into clean tokens\n",
    "def clean_text(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    tokens = [w.translate(table) if eos not in w else w for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic or not end of sentence tag\n",
    "    tokens = [word for word in tokens if word.isalpha() or eos in word]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Load the data**\n",
    "The training data for this example is Yelp review data. First we load the data from Turi Create.\n",
    "Then we parse and clean the text, creating sequences of 11 words. The first 10 words in the sequence will be fed to\n",
    "the neural network as input, and the 11th word will be used as output. We also perform tokenization which maps each word to a unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small sample of user reviews from a yelp dataset\n",
    "data = tc.SFrame('https://static.turi.com/datasets/regression/yelp-data.csv')['text'].sample(.01) # grab only 1% for this example\n",
    "print(f'\\n\\nLoaded {len(data)} text entries from the Yelp review dataset', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some initial cleaning and then dump all of the text together into a single document\n",
    "full_text = parse_text(data)\n",
    "del(data) # save some space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text and perform tokenization\n",
    "tokens = clean_text(doc=full_text)\n",
    "print('Total Tokens: %d' % len(tokens), flush=True)\n",
    "print('Unique Tokens: %d' % len(set(tokens)), flush=True)\n",
    "print('\\nSample Tokens\\n', tokens[:50], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize into sequences of tokens\n",
    "length = 10 + 1 # 10 words as input, 1 as output\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # check for eos tag in the line\n",
    "    if eos in line:\n",
    "        # if eos tag is the last term in the line - remove it from the end\n",
    "        if line.endswith(eos):\n",
    "            line = line[:-4].strip()\n",
    "        # same thing if it's the first\n",
    "        elif line.startswith(eos):\n",
    "            line = line[4:].strip()\n",
    "        else:\n",
    "            try:\n",
    "                front, back = line.split(eos)\n",
    "                if len(front) > len(back):\n",
    "                    line = front.strip()\n",
    "                else:\n",
    "                    line = back.strip()\n",
    "            except:\n",
    "                # skip it if for some reason this fails - we got plenty of data\n",
    "                continue\n",
    "    # store line with others\n",
    "    sequences.append(line)\n",
    "print(f'Total Sequences: {len(sequences)}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequences of words as integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "tokenized_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "max_sequence_len = max([len(x) for x in tokenized_sequences])\n",
    "input_sequences = array(pad_sequences(\n",
    "    tokenized_sequences,\n",
    "    maxlen=max_sequence_len,\n",
    "    padding='pre'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f'{vocab_size} total unique words in our training data corpus', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our tokenized sequences (notice the integer values instead of raw text)\n",
    "input_sequences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate sequences into input and output (X and y)\n",
    "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_length))   # Docs: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "model.add(LSTM(units=128))                                                           # Docs: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "model.add(Dense(128, activation='relu'))                                             # Docs: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary(), flush=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model - this might take a while. For better results, train for additional epochs\n",
    "model.fit(X, y, batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickup training from where you left off last with the following\n",
    "# Using an initial_epoch of 5 and epochs of 10, the model will begin at epoch 6 and train up until it reaches 10 (from where you last left off)\n",
    "#model.fit(X, y, batch_size=256, initial_epoch=5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the tokenizer map so we can lookup a word by it's index\n",
    "index_word_lookup = dict([[v,k] for k,v in tokenizer.word_index.items()])\n",
    "\n",
    "# Function to generate new text based on the input\n",
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = pad_sequences(\n",
    "            sequences=tokenizer.texts_to_sequences([seed_text]),\n",
    "            maxlen=max_sequence_len-1,\n",
    "            padding='pre'\n",
    "        )\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        # Generate the output word\n",
    "        seed_text += \" \" + index_word_lookup[predicted[0]]\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the language model by passing in some seed text and the number of words\n",
    "generate_text(\"I think that I\", 5, length, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Save the model**\n",
    "Once your model has been created, it must be exported to Core ML format so it can be used by your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the language model to Core ML format\n",
    "coreml_model_name = \"WordLanguageModel.mlmodel\"\n",
    "coreml_model = coremltools.converters.keras.convert(\n",
    "    model,\n",
    "    input_names=['tokenizedInputSeq'],\n",
    "    output_names=['tokenProbs']\n",
    ")\n",
    "# Add description information (if you want) and export\n",
    "coreml_model.short_description = 'Predicts the most likely next word given a string of text'\n",
    "coreml_model.input_description['tokenizedInputSeq'] = 'An array of 10 tokens according to a pre-deifned mapping'\n",
    "coreml_model.output_description['tokenProbs'] = 'An array of token probabilities across the entire vocabular'\n",
    "coreml_model.save(coreml_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the model in your app\n",
    "If you haven't configured your Skafos project to handle Core ML delivery to your app, make sure to do that by entering the proper ID's and Keys on your project page of the dashboard. Follow along with the integration guide from there (you will see the link).\n",
    "\n",
    "Instead of downloading one of the pre-trained models in the integration guide, go ahead and download the `.mlmodel` that you just trained and converted to Core ML. Drag it to your app's Xcode project. Another important thing is that you will need to include the 'tokenizer.word_index` and `index_word_lookup` dictionaries somewhere in your app so it can translate text to int and vice versa.\n",
    "\n",
    "Moving forward, as you retrain and update your model, you **won't** need to do that step. You can just save it using the Skafos SDK below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (push to device) model through the Skafos SDK\n",
    "## This will trigger an update to your app if you have configed your app with Skafos framework and have downloaded the initial .mlmodel to Xcode\n",
    "\n",
    "with open(coreml_model_name, 'rb') as model_data:\n",
    "    # load the coreml model from disk\n",
    "    model_obj = model_data.read()\n",
    "    # save through the skafos sdk\n",
    "    res = ska.engine.save_model(\n",
    "        coreml_model_name,\n",
    "        model_obj,\n",
    "        tags=['latest'],\n",
    "        access='public'\n",
    "    )\n",
    "    # print the result\n",
    "    print(res.result(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index_work_lookup.json', 'w') as fp:\n",
    "    json.dump(index_word_lookup, fp)\n",
    "with open('word_index_lookup.json', 'w') as fp:\n",
    "    json.dump(tokenizer.word_index, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it here, great work! Another blog post in the future will show how to build an iOS application to use this model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
